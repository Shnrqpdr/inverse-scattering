{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse scattering for circular billiard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.special as sc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/dados.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot da scattering cross length para gamma, R = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R_2 = df[df['R'] == 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_gamma_R_2 = df_R_2[(df_R_2['gamma'] > 1.97) & (df_R_2['gamma'] < 2.03)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_gamma_R_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_array = row_gamma_R_2.drop(columns=['M', 'HBAR', 'k_min', 'k_max', 'delta_k', 'n_min', 'n_max', 'gamma', 'R']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_min = 0.02\n",
    "k_max = 3.0\n",
    "k = np.linspace(k_min, k_max, 596)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "axes.plot(k, l_array[0], ls='-', label=f\"$\\gamma = 1.97$\")\n",
    "axes.plot(k, l_array[1], ls='-', label=f\"$\\gamma = 1.98$\")\n",
    "axes.plot(k, l_array[2], ls='-', label=f\"$\\gamma = 1.99$\")\n",
    "axes.plot(k, l_array[3], ls='-', label=f\"$\\gamma = 2.00$\")\n",
    "axes.plot(k, l_array[4], ls='-', label=f\"$\\gamma = 2.01$\")\n",
    "axes.plot(k, l_array[5], ls='-', label=f\"$\\gamma = 2.02$\")\n",
    "axes.set_title(f'Scattering cross length values')\n",
    "axes.set_xlabel(f'k')\n",
    "axes.legend(loc='upper right')\n",
    "plt.grid(linestyle='-', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação da rede neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição dos inputs da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(columns=['gamma', 'R'])\n",
    "\n",
    "features = features.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = df[['gamma', 'R']]\n",
    "\n",
    "targets = targets.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da rede neural\n",
    "\n",
    "A rede neural implementada é uma rede neural do tipo Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hidden_neurons(input_size, output_size):\n",
    "    # Rule 1: The number of hidden neurons should be between the size of the input layer and the size of the output layer\n",
    "    rule_1 = max(input_size, output_size)\n",
    "    \n",
    "    # Rule 2: The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer\n",
    "    rule_2 = int(2/3 * input_size + output_size)\n",
    "    \n",
    "    # Rule 3: The number of hidden neurons should be less than twice the size of the input layer\n",
    "    rule_3 = min(2 * input_size - 1, input_size + input_size // 3)\n",
    "    \n",
    "    # The number of hidden neurons should be the minimum that satisfies all rules, so will be the input size + 1/3*input_size\n",
    "    hidden_neurons = int(input_size + input_size/3)\n",
    "\n",
    "    print(hidden_neurons)\n",
    "    return 201\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_neurons = calculate_hidden_neurons(input_size, output_size)\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(input_size, hidden_neurons)\n",
    "        self.output_layer = nn.Linear(hidden_neurons, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # def add_hidden_layer(self):\n",
    "    #     # Determine the size of the new hidden layer\n",
    "    #     if len(self.hidden_layers) == 0:\n",
    "    #         hidden_size = calculate_hidden_neurons(self.input_size, self.output_size)\n",
    "    #         new_layer = nn.Linear(self.input_size, hidden_size)\n",
    "    #     else:\n",
    "    #         prev_hidden_size = self.hidden_layers[-1].out_features\n",
    "    #         hidden_size = calculate_hidden_neurons(prev_hidden_size, self.output_size)\n",
    "    #         new_layer = nn.Linear(prev_hidden_size, hidden_size)\n",
    "    #     self.hidden_layers.append(new_layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.hidden_layer(x))\n",
    "        x = self.sigmoid(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, criterion, test_loader):\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "    print(f'Test Loss: {test_loss/len(test_loader)}')\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis para o modelo, separação dos dados de teste e treino e etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_data = train_test_split(features, test_size = 0.2, random_state = 2)\n",
    "real_train_data, validation_data = train_test_split(train, test_size = 0.3, random_state = 2)\n",
    "\n",
    "train_target, test_target = train_test_split(targets, test_size = 0.2, random_state = 2)\n",
    "real_train_target, validation_target = train_test_split(train_target, test_size = 0.3, random_state = 2)\n",
    "\n",
    "input_size = 603\n",
    "output_size = 2\n",
    "batch_size = 200\n",
    "num_epochs = 150\n",
    "learning_rate = 0.001\n",
    "\n",
    "data_train = torch.tensor(real_train_data, dtype=torch.float32)\n",
    "target_train = torch.tensor(real_train_target, dtype=torch.float32)\n",
    "\n",
    "data_val = torch.tensor(validation_data, dtype=torch.float32)\n",
    "target_val = torch.tensor(validation_target, dtype=torch.float32)\n",
    "\n",
    "data_test = torch.tensor(test_data, dtype=torch.float32)\n",
    "target_test = torch.tensor(test_target, dtype=torch.float32)\n",
    "\n",
    "print(\"Input Shapes:\")\n",
    "print(data_train.shape, data_val.shape, data_test.shape)\n",
    "print(\"Target Shapes:\")\n",
    "print(target_train.shape, target_val.shape, target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "train_dataset = torch.utils.data.TensorDataset(data_train, target_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(data_val, target_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(data_test, target_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs)\n",
    "evaluate_model(model, criterion, test_loader)\n",
    "\n",
    "torch.save(model.state_dict(), 'mlp_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizedMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_hidden_layers):\n",
    "        super(GeneralizedMLP, self).__init__()\n",
    "        hidden_neurons = calculate_hidden_neurons(input_size, output_size)\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(input_size, hidden_neurons))\n",
    "        \n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_neurons, hidden_neurons))\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_neurons, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.sigmoid(layer(x))\n",
    "\n",
    "        x = self.sigmoid(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_state_dict = torch.load('mlp_weights.pth')\n",
    "\n",
    "# Initialize the new model\n",
    "extended_model = GeneralizedMLP(input_size, output_size, 2)\n",
    "\n",
    "# Copy the weights of the hidden_layer to hidden_layer1\n",
    "\n",
    "extended_model.hidden_layers[0].load_state_dict({\n",
    "    'weight': saved_state_dict['hidden_layer.weight'],\n",
    "    'bias': saved_state_dict['hidden_layer.bias']\n",
    "})\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(extended_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_model(extended_model, criterion, optimizer, train_loader, val_loader, num_epochs)\n",
    "evaluate_model(extended_model, criterion, test_loader)\n",
    "\n",
    "torch.save(extended_model.state_dict(), 'generalized_mlp_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_state_dict = torch.load('generalized_mlp_weights.pth')\n",
    "\n",
    "extended_model = GeneralizedMLP(input_size, output_size, 3)\n",
    "\n",
    "extended_model.hidden_layers[0].load_state_dict({\n",
    "    'weight': saved_state_dict['hidden_layers.0.weight'],\n",
    "    'bias': saved_state_dict['hidden_layers.0.bias']\n",
    "})\n",
    "\n",
    "for i in range(1, 2):\n",
    "    extended_model.hidden_layers[i].load_state_dict({\n",
    "        'weight': saved_state_dict[f'hidden_layers.{i}.weight'],\n",
    "        'bias': saved_state_dict[f'hidden_layers.{i}.bias']\n",
    "})\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(extended_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_model(extended_model, criterion, optimizer, train_loader, val_loader, num_epochs)\n",
    "evaluate_model(extended_model, criterion, test_loader)\n",
    "\n",
    "torch.save(extended_model.state_dict(), 'generalized_mlp_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_state_dict = torch.load('generalized_mlp_weights.pth')\n",
    "\n",
    "extended_model = GeneralizedMLP(input_size, output_size, 4)\n",
    "\n",
    "extended_model.hidden_layers[0].load_state_dict({\n",
    "    'weight': saved_state_dict['hidden_layers.0.weight'],\n",
    "    'bias': saved_state_dict['hidden_layers.0.bias']\n",
    "})\n",
    "\n",
    "for i in range(1, 3):\n",
    "    extended_model.hidden_layers[i].load_state_dict({\n",
    "        'weight': saved_state_dict[f'hidden_layers.{i}.weight'],\n",
    "        'bias': saved_state_dict[f'hidden_layers.{i}.bias']\n",
    "})\n",
    "    \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(extended_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_model(extended_model, criterion, optimizer, train_loader, val_loader, num_epochs)\n",
    "evaluate_model(extended_model, criterion, test_loader)\n",
    "\n",
    "torch.save(extended_model.state_dict(), 'generalized_mlp_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse-scat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
